{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python 機器學習期末專案\n",
    "\n",
    "**成員: 蔡宇軒、張鈺欣、楊雅嵐** <br>\n",
    "**題目: 韓國女團人臉辨識**<br>\n",
    "\n",
    "## 介紹\n",
    "\n",
    "近幾年，韓國文化與歌曲逐漸流行，韓國藝人在台知名度也大增。我們常常能看到韓國女團在舞台上勁歌熱舞，他們都有著不可思議美麗的臉龐，令人羨慕不已。然而，我們卻總能在他們的臉龐中發現些微的相似。如今，整形已是公開的秘密，技術也越來越進步，我們也常聽到新人女團常常以神似某些當紅女團的成員作為宣傳的手法。同樣的，隨著科技日新月異，人臉辨識的技術也逐漸發展中。我們希望建好的model不但能辨識出人臉的差異，也能辨別兩人是否有高度的相似度。\n",
    "\n",
    "## 實作方法\n",
    "\n",
    "**<p style=\"color:red;\">成果展示每個階段都有錄影片，可以點進去觀看哦**\n",
    "\n",
    "**第一階段:**\n",
    "data 已經收集大概每個女星 600 張照片，總共有5個女星，總共有3000張照片。然後透過調整明亮度和水平翻轉圖片，將圖片擴展成9000張。\n",
    "而data的收集方法有3:第一種是去搜尋女星的名字，下載照片。第二種方式是去找女星的video，第三種是去IG下載。從中取出適合的frame做為data照片。(適合代表不希望有其他人的臉，因為怕偵測到其他人的臉孔。所以取出frame後如果有其他人的臉孔，我們就要自己一張張切割照片取出目標女星的臉。)\n",
    "\n",
    "第一階段做出的成果是結合interact，來偵測圖片中的女星是誰，詳情見**結果呈現1: 利用interact展示成果**\n",
    "\n",
    "**第二階段**\n",
    "這個階段我們希望達成的成果是在影片中辨識出這五位女星與其他人，因此我們又去收集了Data，因為需要增加一個類別是\"其他人\"。這次我們利用的是網路現有的DataSet，UTKFace，有許多不同的人臉，包括男性女性老人嬰兒，我們取出其中的8649張圖片，全部放在Others資料夾，當作是一個新的類別，因此現在的類別有六項，也就是五位女星加上一類其他人，總共有17223筆Data。\n",
    "\n",
    "最後的呈現成果見**結果呈現4: 影片 real-time偵測人臉 =>多人在影片中的人臉識別**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/ZWyisGt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: 找到資料夾內所有的檔案\n",
    "\n",
    "這裡把不同Label(不同女星)的Data存在不同的資料夾，這樣就不需要用檔名去label，而是用資料夾去Label，因此第一步先把5位女星資料夾的data分別讀進陣列的5個index\n",
    "\n",
    "![](https://i.imgur.com/ZguC5v0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(5):\\n    for f in files[i]:\\n        print(f)\\n    print('----')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = ['.\\images\\M','.\\images\\J','.\\images\\\\N','.\\images\\S','.\\images\\Z']\n",
    "files = [[],[],[],[],[]]\n",
    "# r=root, d=directories, f = files\n",
    "for index in range(5):\n",
    "    for r, d, f in os.walk(path[index]):\n",
    "        for file in f:\n",
    "            files[index].append(os.path.join(r, file))\n",
    "'''\n",
    "for i in range(5):\n",
    "    for f in files[i]:\n",
    "        print(f)\n",
    "    print('----')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Preprocessing\n",
    "\n",
    "因為Data數量大概有600張照片\\*5位女星，大概是3000張照片，所以我們用兩種方式讓Data可以增加: <br>\n",
    "1.圖片水平反轉<br>\n",
    "2.圖片明亮度改變<br>\n",
    "因為電腦與人不同，看到1或是2的轉變就會覺得是很不一樣的相片，因此用此方式來讓Data增加三倍，大約是9000張圖片<br>\n",
    "\n",
    "![](https://i.imgur.com/s4MrMWL.png)\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "照片讀進來並做以上的處理，將一張圖片變成3張圖片之後，會用face_recognition這個Library中的face_recognition.face_encodings。這個API的功用是可以把一張圖片的人臉偵測出來，然後把五官偵測出來，最後把五官變成是一個128-dim的vector，具體來說就是一張圖片餵進來，會自動算出描述五官的特徵向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加明亮度\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:8876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 最後要記錄出的樣品向量(128 維度) 和標籤(明星名)\n",
    "encodinglist = []\n",
    "labels = []\n",
    "# 一橫列畫幾個人\n",
    "width = 5\n",
    "count=0\n",
    "for index_of_person in range(5):\n",
    "    # 總共幾個橫列\n",
    "    height = int(len(files[index_of_person]) / width) + 1\n",
    "    # 整個大圖的size\n",
    "    plt.figure(figsize=(100,100))\n",
    "    #print('------------')\n",
    "    for (i, f) in enumerate(files[index_of_person]):\n",
    "        #if i >540: \n",
    "        #if f == (\".\\images\\S\\SinB.full.160333.jpg\"): \n",
    "        # Step1. 讀取檔案\n",
    "        for k in range(3):\n",
    "                if(k==0):\n",
    "                    img = face_recognition.load_image_file(f)\n",
    "                elif(k==1):\n",
    "                    img = face_recognition.load_image_file(f)\n",
    "                    img = cv2.flip(img, 1)\n",
    "                else:\n",
    "                    img = face_recognition.load_image_file(f)\n",
    "                    img = increase_brightness(img, value=80)\n",
    "                    \n",
    "            # Step2. 把臉的降維向量算出, 用已經做好的cnn, [0] 第一張臉\n",
    "                face_detect_and_encoding = face_recognition.face_encodings(img)\n",
    "                #print(face_encoding[0])\n",
    "            #如果沒偵測到臉，就不用特徵向量\n",
    "                if len(face_detect_and_encoding) <= 0:\n",
    "                    #print(\"No faces found in the image!\")\n",
    "                    continue\n",
    "                else :\n",
    "                    face_encoding = face_detect_and_encoding[0]\n",
    "\n",
    "                #print(f+\", NO:\"+str(count))\n",
    "                count=count+1\n",
    "\n",
    "                #height,width = img.shape[:2] #get image height and width\n",
    "                #if(height>1500 or width>1500):\n",
    "                #    img = cv2.resize(img,(width//3,height//3),interpolation=cv2.INTER_CUBIC)\n",
    "                #elif(height>800 or width>800):\n",
    "                #    img = cv2.resize(img,(width//2,height//2),interpolation=cv2.INTER_CUBIC)\n",
    "                #(top, right, bottom, left) = face_recognition.face_locations(img, model=\"cnn\")[0]\n",
    "\n",
    "                #draw\n",
    "                #size = int(img.shape[0] / 100)\n",
    "                #cv2.rectangle(img, (left, top), (right, bottom), (100, 100, 200),thickness=3)\n",
    "                #print(str(right) + ' '+str(top) + ' '+str(bottom) + ' '+str(left))\n",
    "            # Step4. 把它加到我自己準備的list 裡\n",
    "                encodinglist.append(face_encoding)\n",
    "            # Step6. 把人名到我自己準備的list\n",
    "                labels.append(index_of_person)\n",
    "        # 利用enumerate 得到的i 指定subplot\n",
    "            #plt.subplot(height, width, i + 1)\n",
    "            #plt.axis(\"off\")\n",
    "            #plt.imshow(img)\n",
    "print(\"Total:\"+str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Model 建立\n",
    "\n",
    "這一步我們總共用了3個Model來實現人臉辨識，以下將一一說明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"color:red;\"> 1. CNN Model \n",
    "我們建構的CNN model 結構:\n",
    "![](https://i.imgur.com/eqQkrn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam ,SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain長度: 7988\n"
     ]
    }
   ],
   "source": [
    "# 秀一下降維過後的向量\n",
    "#print(\"x長度:\", len(encodinglist))\n",
    "#print(\"y長度:\", len(labels))\n",
    "\n",
    "data_length = len(encodinglist)\n",
    "\n",
    "data_x = np.array(encodinglist)\n",
    "data_y = np.array(labels)\n",
    "\n",
    "data_x = data_x.reshape(data_length, 128, 1, 1)\n",
    "\n",
    "data_y = np_utils.to_categorical(data_y, 5)\n",
    "\n",
    "x_train,x_test,y_train,y_test =train_test_split( data_x , data_y ,test_size = 0.1, random_state = 4)\n",
    "\n",
    "\n",
    "print(\"xtrain長度:\", len(y_train))\n",
    "#print(\"y長度:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上已經把training data 的 128維都已經存在array之中，接下來開始做CNN判斷\n",
    "\n",
    "TODO: input normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 128, 1, 40)        200       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128, 1, 40)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 64, 1, 40)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 1, 60)         9660      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64, 1, 60)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 1, 60)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 1, 80)         19280     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32, 1, 80)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 16, 1, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 1, 100)        32100     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 755       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 182,145\n",
      "Trainable params: 182,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7988 samples, validate on 7988 samples\n",
      "Epoch 1/30\n",
      "7988/7988 [==============================] - 26s 3ms/step - loss: 0.1598 - acc: 0.9362 - val_loss: 0.0695 - val_acc: 0.9748\n",
      "Epoch 2/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0661 - acc: 0.9753 - val_loss: 0.0593 - val_acc: 0.9775\n",
      "Epoch 3/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0554 - acc: 0.9797 - val_loss: 0.0365 - val_acc: 0.9863\n",
      "Epoch 4/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0513 - acc: 0.9802 - val_loss: 0.0342 - val_acc: 0.9882\n",
      "Epoch 5/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0456 - acc: 0.9836 - val_loss: 0.0290 - val_acc: 0.9903\n",
      "Epoch 6/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0438 - acc: 0.9841 - val_loss: 0.0474 - val_acc: 0.9821\n",
      "Epoch 7/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0405 - acc: 0.9840 - val_loss: 0.0336 - val_acc: 0.9867\n",
      "Epoch 8/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0398 - acc: 0.9849 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 9/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0346 - acc: 0.9875 - val_loss: 0.0232 - val_acc: 0.9911\n",
      "Epoch 10/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0345 - acc: 0.9870 - val_loss: 0.0229 - val_acc: 0.9910\n",
      "Epoch 11/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0335 - acc: 0.9882 - val_loss: 0.0275 - val_acc: 0.9895\n",
      "Epoch 12/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0298 - acc: 0.9888 - val_loss: 0.0260 - val_acc: 0.9905\n",
      "Epoch 13/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0289 - acc: 0.9896 - val_loss: 0.0429 - val_acc: 0.9847\n",
      "Epoch 14/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0268 - acc: 0.9905 - val_loss: 0.0164 - val_acc: 0.9941\n",
      "Epoch 15/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0289 - acc: 0.9895 - val_loss: 0.0212 - val_acc: 0.9932\n",
      "Epoch 16/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0243 - acc: 0.9906 - val_loss: 0.0435 - val_acc: 0.9861\n",
      "Epoch 17/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0237 - acc: 0.9915 - val_loss: 0.0206 - val_acc: 0.9928\n",
      "Epoch 18/30\n",
      "7988/7988 [==============================] - 26s 3ms/step - loss: 0.0223 - acc: 0.9916 - val_loss: 0.0190 - val_acc: 0.9930\n",
      "Epoch 19/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0214 - acc: 0.9923 - val_loss: 0.0105 - val_acc: 0.9963\n",
      "Epoch 20/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0212 - acc: 0.9926 - val_loss: 0.0175 - val_acc: 0.9937\n",
      "Epoch 21/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0169 - acc: 0.9936 - val_loss: 0.0129 - val_acc: 0.9952\n",
      "Epoch 22/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0212 - acc: 0.9922 - val_loss: 0.0127 - val_acc: 0.9955\n",
      "Epoch 23/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0208 - val_acc: 0.9934\n",
      "Epoch 24/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0161 - acc: 0.9944 - val_loss: 0.0086 - val_acc: 0.9966\n",
      "Epoch 25/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0200 - acc: 0.9931 - val_loss: 0.0113 - val_acc: 0.9956\n",
      "Epoch 26/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.0121 - val_acc: 0.9958\n",
      "Epoch 27/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0192 - val_acc: 0.9932\n",
      "Epoch 28/30\n",
      "7988/7988 [==============================] - 24s 3ms/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0099 - val_acc: 0.9965\n",
      "Epoch 29/30\n",
      "7988/7988 [==============================] - 25s 3ms/step - loss: 0.0166 - acc: 0.9947 - val_loss: 0.0095 - val_acc: 0.9966\n",
      "Epoch 30/30\n",
      "7988/7988 [==============================] - 26s 3ms/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0098 - val_acc: 0.9964\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(40, (4,1), padding='same', input_shape=(128,1,1))) #4個filter，都是5*5\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model1.add(Conv2D(60, (4,1), padding='same'))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model1.add(Conv2D(80, (4,1), padding='same'))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model1.add(Conv2D(100, (4,1), padding='same'))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(150))#拉平完送進最後一個普通NN\n",
    "model1.add(Activation('relu'))\n",
    "\n",
    "model1.add(Dense(5))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "model1_out=model1.fit(x_train, y_train, batch_size=3, epochs=30,verbose =1,validation_data = (x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_json = model1.to_json()\n",
    "open('model1.json', 'w').write(model1_json)\n",
    "model1.save_weights('model1_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"color:red;\"> 2. Normal NN(Fully-connected)\n",
    "我們建構的NN model 結構:\n",
    "![](https://i.imgur.com/mUABOij.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x長度: 8876\n",
      "y長度: 8876\n",
      "xtrain長度: 7988\n"
     ]
    }
   ],
   "source": [
    "# 秀一下降維過後的向量\n",
    "print(\"x長度:\", len(encodinglist))\n",
    "print(\"y長度:\", len(labels))\n",
    "\n",
    "data_length = len(encodinglist)\n",
    "\n",
    "NN_inputx = np.array(encodinglist)\n",
    "NN_inputy = np.array(labels)\n",
    "\n",
    "NN_inputy = np_utils.to_categorical(NN_inputy, 5)\n",
    "\n",
    "NN_x_train,NN_x_test,NN_y_train,NN_y_test =train_test_split( NN_inputx , NN_inputy ,test_size = 0.1, random_state = 4)\n",
    "\n",
    "\n",
    "print(\"xtrain長度:\", len(y_train))\n",
    "#print(\"y長度:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 200)               25800     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 46,405\n",
      "Trainable params: 46,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7988 samples, validate on 7988 samples\n",
      "Epoch 1/30\n",
      "7988/7988 [==============================] - 17s 2ms/step - loss: 0.0350 - acc: 0.8877 - val_loss: 0.0157 - val_acc: 0.9465\n",
      "Epoch 2/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0159 - acc: 0.9501 - val_loss: 0.0144 - val_acc: 0.9526\n",
      "Epoch 3/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0146 - acc: 0.9512 - val_loss: 0.0093 - val_acc: 0.9720\n",
      "Epoch 4/30\n",
      "7988/7988 [==============================] - 16s 2ms/step - loss: 0.0137 - acc: 0.9537 - val_loss: 0.0106 - val_acc: 0.9643\n",
      "Epoch 5/30\n",
      "7988/7988 [==============================] - 18s 2ms/step - loss: 0.0130 - acc: 0.9562 - val_loss: 0.0095 - val_acc: 0.9688\n",
      "Epoch 6/30\n",
      "7988/7988 [==============================] - 16s 2ms/step - loss: 0.0124 - acc: 0.9587 - val_loss: 0.0160 - val_acc: 0.9474\n",
      "Epoch 7/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0112 - acc: 0.9646 - val_loss: 0.0117 - val_acc: 0.9618\n",
      "Epoch 8/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0119 - acc: 0.9604 - val_loss: 0.0109 - val_acc: 0.9642\n",
      "Epoch 9/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0110 - acc: 0.9642 - val_loss: 0.0071 - val_acc: 0.9758\n",
      "Epoch 10/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0102 - acc: 0.9666 - val_loss: 0.0094 - val_acc: 0.9683\n",
      "Epoch 11/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0106 - acc: 0.9659 - val_loss: 0.0070 - val_acc: 0.9775\n",
      "Epoch 12/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0095 - acc: 0.9678 - val_loss: 0.0078 - val_acc: 0.9731\n",
      "Epoch 13/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0094 - acc: 0.9692 - val_loss: 0.0072 - val_acc: 0.9773\n",
      "Epoch 14/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0096 - acc: 0.9678 - val_loss: 0.0140 - val_acc: 0.9544\n",
      "Epoch 15/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0089 - acc: 0.9725 - val_loss: 0.0073 - val_acc: 0.9758\n",
      "Epoch 16/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0092 - acc: 0.9698 - val_loss: 0.0087 - val_acc: 0.9728\n",
      "Epoch 17/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0083 - acc: 0.9732 - val_loss: 0.0064 - val_acc: 0.9798\n",
      "Epoch 18/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0082 - acc: 0.9725 - val_loss: 0.0114 - val_acc: 0.9638\n",
      "Epoch 19/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0084 - acc: 0.9725 - val_loss: 0.0082 - val_acc: 0.9720\n",
      "Epoch 20/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0085 - acc: 0.9718 - val_loss: 0.0061 - val_acc: 0.9806\n",
      "Epoch 21/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0079 - acc: 0.9736 - val_loss: 0.0075 - val_acc: 0.9750\n",
      "Epoch 22/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0072 - acc: 0.9770 - val_loss: 0.0051 - val_acc: 0.9835\n",
      "Epoch 23/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0077 - acc: 0.9737 - val_loss: 0.0089 - val_acc: 0.9710\n",
      "Epoch 24/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0070 - acc: 0.9780 - val_loss: 0.0057 - val_acc: 0.9808\n",
      "Epoch 25/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0068 - acc: 0.9777 - val_loss: 0.0076 - val_acc: 0.9748\n",
      "Epoch 26/30\n",
      "7988/7988 [==============================] - 15s 2ms/step - loss: 0.0068 - acc: 0.9793 - val_loss: 0.0072 - val_acc: 0.9775\n",
      "Epoch 27/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0069 - acc: 0.9786 - val_loss: 0.0046 - val_acc: 0.9859\n",
      "Epoch 28/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0064 - acc: 0.9795 - val_loss: 0.0069 - val_acc: 0.9762\n",
      "Epoch 29/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0062 - acc: 0.9802 - val_loss: 0.0054 - val_acc: 0.9822\n",
      "Epoch 30/30\n",
      "7988/7988 [==============================] - 14s 2ms/step - loss: 0.0059 - acc: 0.9815 - val_loss: 0.0070 - val_acc: 0.9782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(200, input_dim=128))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Activation(\"relu\"))\n",
    "model2.add(Dense(5))\n",
    "model2.add(Activation(\"softmax\"))\n",
    "model2.compile(loss='mse', optimizer='Adam', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "model2_out = model2.fit(NN_x_train, NN_y_train,\n",
    "                        batch_size = 3,\n",
    "                        epochs = 30,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (NN_x_train, NN_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_json = model2.to_json()\n",
    "open('model2.json', 'w').write(model2_json)\n",
    "model2.save_weights('model2_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <p style=\"color:red;\"> 3. Euclidean Distance\n",
    "![](https://i.imgur.com/rp63F6b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x長度: 8876\n",
      "y長度: 8876\n",
      "x長度: 7988\n",
      "y長度: 888\n"
     ]
    }
   ],
   "source": [
    "# 秀一下降維過後的向量\n",
    "print(\"x長度:\", len(encodinglist))\n",
    "print(\"y長度:\", len(labels))\n",
    "\n",
    "data_length = len(encodinglist)\n",
    "\n",
    "E_inputx = np.array(encodinglist)\n",
    "E_inputy = np.array(labels)\n",
    "E_x_train,E_x_test,E_y_train,E_y_test =train_test_split( E_inputx , E_inputy ,test_size = 0.1, random_state = 4)\n",
    "\n",
    "\n",
    "print(\"x長度:\", len(y_train))\n",
    "print(\"y長度:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean0,mean1,mean2,mean3,mean4=[],[],[],[],[]\n",
    "for i in range(len(E_y_train)):\n",
    "    if ((E_y_train[i]==0)):\n",
    "        if(len(mean0)==0):\n",
    "            mean0.append(E_x_train[i])\n",
    "        else:\n",
    "            mean0+=E_x_train[i]\n",
    "    elif (E_y_train[i]==1):\n",
    "        if(len(mean1)==0):\n",
    "            mean1.append(E_x_train[i])\n",
    "        else:\n",
    "            mean1+=E_x_train[i]\n",
    "    elif (E_y_train[i]==2):\n",
    "        if(len(mean2)==0):\n",
    "            mean2.append(E_x_train[i])\n",
    "        else:\n",
    "            mean2+=E_x_train[i]\n",
    "    elif (E_y_train[i]==3):\n",
    "        if(len(mean3)==0):\n",
    "            mean3.append(E_x_train[i])\n",
    "        else:\n",
    "            mean3+=E_x_train[i]\n",
    "    elif (E_y_train[i]==4):\n",
    "        if(len(mean4)==0):\n",
    "            mean4.append(E_x_train[i])\n",
    "        else:\n",
    "            mean4+=E_x_train[i]\n",
    "            \n",
    "mean0/=len(encodinglist)\n",
    "mean1/=len(encodinglist)\n",
    "mean2/=len(encodinglist)\n",
    "mean3/=len(encodinglist)\n",
    "mean4/=len(encodinglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面已經取出5位女星中每個人的128-dim的elementwise平均了，以下用TestData一個一個試與誰最近預測出是誰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance 正確率: 0.3704954954954955 ,正確:329 ,錯誤:559\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "right=0\n",
    "wrong=0\n",
    "for i in range(len(E_y_test)):\n",
    "    dst0 = distance.euclidean(mean0,E_x_test[i])\n",
    "    dst1 = distance.euclidean(mean1,E_x_test[i])\n",
    "    dst2 = distance.euclidean(mean2,E_x_test[i])\n",
    "    dst3 = distance.euclidean(mean3,E_x_test[i])\n",
    "    dst4 = distance.euclidean(mean4,E_x_test[i])\n",
    "    dst = [dst0,dst1,dst2,dst3,dst4]\n",
    "    predict = dst.index(min(dst))\n",
    "    #print(str(dst0)+\" , \"+str(dst1)+\" , \"+str(dst2)+\" , \"+str(dst3)+\" , \"+str(dst4))\n",
    "    #print(\"預測: \"+str(predict)+\" , 真實: \",str(E_y_test[i]))\n",
    "    if(predict==E_y_test[i]):\n",
    "        right+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "print(\"Euclidean Distance 正確率: \"+str(right/(right+wrong))+\" ,正確:\"+str(right)+\" ,錯誤:\"+str(wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\">上面可以發現CNN表現是最好的，可以達到0.99以上NN次之，大概在0.97~0.98。Euclidean Distance則是不太符合預期，只有0.37。\n",
    "\n",
    "## 結果呈現\n",
    "\n",
    "一樣需要把Testdata經由前處理降維成128-dim的向量。才餵進去我們的Model中，這裡不在上面Split testing data是因為方便等下做結果的展示，所以才把 Testing data分開資料夾放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Testpath = ['.\\images\\Testdata\\M','.\\images\\Testdata\\J','.\\images\\Testdata\\\\N','.\\images\\Testdata\\S','.\\images\\Testdata\\Z']\n",
    "Testfiles = [[],[],[],[],[]]\n",
    "# r=root, d=directories, f = files\n",
    "for index in range(5):\n",
    "    for r, d, f in os.walk(Testpath[index]):\n",
    "        for file in f:\n",
    "            Testfiles[index].append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import random\n",
    "mpl.rc('font',family ='Noto Sans CJK TC')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%env KERAS_BACKEND=tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "import face_recognition\n",
    "from keras.utils import np_utils\n",
    "\n",
    "model1 = model_from_json(open('./model1.json').read())\n",
    "model1.load_weights('./model1_weights.h5')\n",
    "model1.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "model2 = model_from_json(open('./model2.json').read())\n",
    "model2.load_weights('./model2_weights.h5')\n",
    "model2.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest長度: 260\n"
     ]
    }
   ],
   "source": [
    "encodinglist = []\n",
    "labels = []\n",
    "# 一橫列畫幾個人\n",
    "width = 5\n",
    "count=0\n",
    "\n",
    "for index_of_person in range(5):\n",
    "    for (i, f) in enumerate(Testfiles[index_of_person]):\n",
    "                \n",
    "                img = face_recognition.load_image_file(f)\n",
    "                face_detect_and_encoding = face_recognition.face_encodings(img)\n",
    "                if len(face_detect_and_encoding) <= 0:\n",
    "                    #print(\"No faces found in the image!\")\n",
    "                    continue\n",
    "                else :\n",
    "                    face_encoding = face_detect_and_encoding[0]\n",
    "                    #print(count)\n",
    "                count=count+1\n",
    "                encodinglist.append(face_encoding)\n",
    "                labels.append(index_of_person)\n",
    "                \n",
    "data_length = len(encodinglist)\n",
    "data_x = np.array(encodinglist)\n",
    "data_y = np.array(labels)\n",
    "x_test = data_x.reshape(data_length, 128, 1, 1)\n",
    "y_test = np_utils.to_categorical(data_y, 5)\n",
    "\n",
    "print(\"xtest長度:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 0s 2ms/step\n",
      "Model1(CNN):  loss: 0.01993767430563961 正確率: 0.9938461597149189\n",
      "260/260 [==============================] - 0s 442us/step\n",
      "Model2(NN):  loss: 0.015858105281907998 正確率: 0.9930769260113056\n",
      "Euclidean Distance 正確率: 0.3704954954954955 ,正確:329 ,錯誤:559\n"
     ]
    }
   ],
   "source": [
    "score_class=model1.evaluate(x_test,y_test)\n",
    "print('Model1(CNN):  loss:',score_class[0],'正確率:',score_class[1])\n",
    "score_class2=model2.evaluate(data_x,y_test)\n",
    "print('Model2(NN):  loss:',score_class2[0],'正確率:',score_class2[1])\n",
    "print(\"Euclidean Distance 正確率: \"+str(right/(right+wrong))+\" ,正確:\"+str(right)+\" ,錯誤:\"+str(wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\"> ** 上面可以看到，在260筆Data之中，CNN還是表現得最好，沒有超出NN很多的原因是因為Testing Data數較少，但上面在Train的時候Model中的Vaildataion accuracy已經明確的告訴我們，CNN在我們case的大數據情況至少高出1%的正確率，因此下面的結果展示還是都用CNN model的結果。**<br>\n",
    "上面Model training結果如下，我將他複製到這裡方便觀看:<br>\n",
    "- NN:loss: 0.0059 - acc: 0.9815 - val_loss: 0.0070 - val_acc: 0.9782<br>\n",
    "- CNN: loss: 0.0182 - acc: 0.9943 - val_loss: 0.0098 - val_acc: 0.9964<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果呈現1: 利用interact展示成果\n",
    "\n",
    "這裡利用interact展示成果，詳情見影片網址:<br>\n",
    "[小力點我](https://www.youtube.com/watch?v=vaujS9WmjJw)<br>\n",
    "使用者按下按鈕後，會從TestData中random的選出一個女星，然後就會對這個女星圖片進行<br>\n",
    "1.把臉找出來並在圖片中框起來<br>\n",
    "2.把五官找出來，並預測人臉是哪位女星<br>\n",
    "\n",
    "在圖片上面顯示的Label，Predict就分別是正確答案和我們預測的答案\n",
    "\n",
    "![](https://i.imgur.com/dBIxRaA.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db8da0197f849078d6357260440c492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "def NameLabel(people):\n",
    "    if(people==0):\n",
    "        return \"敏珠\"\n",
    "    elif(people==1):\n",
    "        return \"Jessica\"\n",
    "    elif(people==2):\n",
    "        return \"娜京\"\n",
    "    elif(people==3):\n",
    "        return \"Sinb\"\n",
    "    else:\n",
    "        return \"子瑜\"\n",
    "\n",
    "def ShowPredict():\n",
    "    TestPeople = random.randint(0,4)\n",
    "    TestImage = random.randint(0,(len(Testfiles[TestPeople])-1))\n",
    "    NowTestfile = Testfiles[TestPeople][TestImage]\n",
    "    print(NowTestfile)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    img = face_recognition.load_image_file(NowTestfile)\n",
    "    \n",
    "    height,width = img.shape[:2] #get image height and width\n",
    "    if(height>1500 or width>1500):\n",
    "        img = cv2.resize(img,(width//3,height//3),interpolation=cv2.INTER_CUBIC)\n",
    "    elif(height>800 or width>800):\n",
    "        img = cv2.resize(img,(width//2,height//2),interpolation=cv2.INTER_CUBIC)\n",
    "   \n",
    "\n",
    "    #=========================perdict face\n",
    "    Testface_encoding = face_recognition.face_encodings(img)\n",
    "    Testdata_x = np.array(Testface_encoding)\n",
    "    #https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image\n",
    "    if(len(Testdata_x)>0):\n",
    "        (top, right, bottom, left) = face_recognition.face_locations(img)[0]\n",
    "        #(top, right, bottom, left) = face_recognition.face_locations(img)[0]\n",
    "        # rectangle(要畫的圖, 左上座標, 右下座標, 顏色, 粗細)\n",
    "        size = int(img.shape[0] / 100)\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (0, 130, 266), size)\n",
    "        \n",
    "        Testdata_x = Testdata_x.reshape(1,128, 1, 1)#注意這裡最前面要加一個1\n",
    "        predict = model1.predict_classes(Testdata_x)\n",
    "        predictLabel = NameLabel(predict[0])\n",
    "    #===========================\n",
    "    \n",
    "        name = NameLabel(TestPeople)\n",
    "\n",
    "        plt.title(\"Label: \" + name +\"   ,   Predict: \"+predictLabel, fontsize=20)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        plt.title(\"偵測不到臉孔或五官\", fontsize=20)\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "    \n",
    "interact_manual(ShowPredict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果呈現2: 影片 real-time偵測人臉 =>單一個人在影片中\n",
    "\n",
    "再來利用影片呈現結果，詳情見影片網址:<br>\n",
    "[小力點我](https://www.youtube.com/watch?v=DKGVdCba3no)<br>\n",
    "\n",
    "這裡對女星的一段影片中的每個Frame，當作是一個圖片的input開始做人臉預測，如果有偵測到人臉的話會在圖片中框出來，並直接在影片中顯示文字，顯示這個Frame預測出她是哪個女星<br>\n",
    "\n",
    "影片中演示了，如果影片越大，預測得就越慢。我們是把影片大小做2倍、3倍等等的縮小。所以必須要在Real-time和影片解析度中間做trade off。\n",
    "\n",
    "![](https://i.imgur.com/ZyHGAlI.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "import face_recognition\n",
    "\n",
    "model1 = model_from_json(open('./model1.json').read())\n",
    "model1.load_weights('./model1_weights.h5')\n",
    "model1.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NameLabel(people):\n",
    "    if(people==0):\n",
    "        return \"Min-Joo\"\n",
    "    elif(people==1):\n",
    "        return \"Jessica\"\n",
    "    elif(people==2):\n",
    "        return \"Na-Gyung\"\n",
    "    elif(people==3):\n",
    "        return \"Sinb\"\n",
    "    else:\n",
    "        return \"Tzu-Yu\"\n",
    "\n",
    "cap = cv2.VideoCapture('./images/Videos/2.mp4')\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "color = (0, 266, 266)\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    height,width = frame.shape[:2]\n",
    "    frame = cv2.resize(frame,(width//3,height//3),interpolation=cv2.INTER_CUBIC)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #=======================================predict face\n",
    "    # (top, right, bottom, left)(top, right, bottom, left) = \n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    #print(len(face_locations))\n",
    "    if(len(face_locations)>0):\n",
    "        (top, right, bottom, left) =  face_locations[0]\n",
    "        # rectangle(要畫的圖, 左上座標, 右下座標, 顏色, 粗細)\n",
    "        size = int(frame.shape[0] / 100)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, size)\n",
    "\n",
    "        #=========================perdict face\n",
    "        \n",
    "        Testface_encoding = face_recognition.face_encodings(frame)\n",
    "        Testdata_x = np.array(Testface_encoding)\n",
    "        #print(Testdata_x.shape[0])\n",
    "        #https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image\n",
    "        if(Testdata_x.shape[0]>0):\n",
    "            Testdata_x = Testdata_x.reshape(1,128, 1, 1)#注意這裡最前面要加一個1\n",
    "            predict = model1.predict_classes(Testdata_x)\n",
    "            predictLabel = NameLabel(predict[0])\n",
    "            cv2.putText(frame, predictLabel, (left+20 , bottom+20 ),font,0.5,color)\n",
    "            #print(predictLabel)\n",
    "        #===========================\n",
    "    \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果呈現3: 影片 real-time偵測人臉 =>Youtube功能\n",
    "這裡多新增了一個功能，就是可以和Youtube一樣，拉bar把影片停在某個Frame，然後可以從那個Frame自動播放下去。<br>\n",
    "[小力點我](https://www.youtube.com/watch?v=m8LfDX0ZAPQ)<br>\n",
    "總共有兩個Bar，Auto只有0和1，是停止和繼續播放的選項。Frame是移動影片進度到某個地方。\n",
    "\n",
    "有趣的是影片中會有例如上一個Frame預測他是Sinb，下一個Frame預測他是Jessica的情況，還有有可能會有雖然找得到臉部的框框，但卻找不到五官的狀況。\n",
    "\n",
    "![](https://i.imgur.com/DtKpF9K.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "import face_recognition\n",
    "\n",
    "model1 = model_from_json(open('./model1.json').read())\n",
    "model1.load_weights('./model1_weights.h5')\n",
    "model1.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NameLabel(people):\n",
    "    if(people==0):\n",
    "        return \"Min-Joo\"\n",
    "    elif(people==1):\n",
    "        return \"Jessica\"\n",
    "    elif(people==2):\n",
    "        return \"Na-Gyung\"\n",
    "    elif(people==3):\n",
    "        return \"Sinb\"\n",
    "    else:\n",
    "        return \"Tzu-Yu\"\n",
    "    \n",
    "frame_no=0\n",
    "cv2.namedWindow('frame', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "cv2.resizeWindow(\"frame\", 640, 480);\n",
    "def ChangeFrame(value):\n",
    "    ;\n",
    "\n",
    "cap = cv2.VideoCapture('./images/Videos/3.mp4')\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "color = (0, 266, 266)\n",
    "frame_lenth = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cv2.createTrackbar('Frame','frame',0,frame_lenth,ChangeFrame)\n",
    "cv2.createTrackbar('Auto','frame',0,1,ChangeFrame)\n",
    "while(cap.isOpened()):\n",
    "    #自動撥放\n",
    "    if(cv2.getTrackbarPos('Auto','frame')==1):\n",
    "        ;\n",
    "    #設定要哪個frame\n",
    "    else:\n",
    "        cap.set(1,cv2.getTrackbarPos('Frame','frame'))\n",
    "        \n",
    "    ret, frame = cap.read()\n",
    "    height,width = frame.shape[:2]\n",
    "    frame = cv2.resize(frame,(width//4,height//4),interpolation=cv2.INTER_CUBIC)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #=======================================predict face\n",
    "    # (top, right, bottom, left)(top, right, bottom, left) = \n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    #print(len(face_locations))\n",
    "    if(len(face_locations)>0):\n",
    "        (top, right, bottom, left) =  face_locations[0]\n",
    "        # rectangle(要畫的圖, 左上座標, 右下座標, 顏色, 粗細)\n",
    "        size = int(frame.shape[0] / 100)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, size)\n",
    "\n",
    "        #=========================perdict face\n",
    "        \n",
    "        Testface_encoding = face_recognition.face_encodings(frame)\n",
    "        Testdata_x = np.array(Testface_encoding)\n",
    "        #print(Testdata_x.shape[0])\n",
    "        #https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image\n",
    "        if(Testdata_x.shape[0]>0):\n",
    "            Testdata_x = Testdata_x.reshape(1,128, 1, 1)#注意這裡最前面要加一個1\n",
    "            predict = model1.predict_classes(Testdata_x)\n",
    "            predictLabel = NameLabel(predict[0])\n",
    "            cv2.putText(frame, predictLabel, (left+20 , bottom+20 ),font,0.5,color)\n",
    "            #print(predictLabel)\n",
    "        #===========================\n",
    "    \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color:red;\">  結果呈現4: 影片 real-time偵測人臉 =>多人在影片中的人臉識別\n",
    "[**小力點我**](https://www.youtube.com/watch?v=s5vHYns1m0U)<br>\n",
    "\n",
    "這裡和前面比起來比較複雜，因為我們想做出的功能是**可以偵測這5位女星的人臉，不是這幾位女星的也都可以標示為其他人**，我們試了以下幾種方法，都是失敗的:<br>\n",
    "1. 去讀CNN的真實output，如果預測出某個女星的真實output值小於某個Threshold，表示我們認為他是這個女星的信心不足，就將他標示為是其他人。\n",
    "2. 用我們稍早捨棄的Euclidean Distance去做輔助，如果Euclidean Distance太大表示距離很大，我們就認為他不是這個女星。將他標示為其他人。\n",
    "\n",
    "以上兩種方法，預測錯誤的機率實在是太高了，因此我們最後決定再度收集Data，這次我們利用的是網路現有的DataSet，[UTKFace](https://susanqq.github.io/UTKFace/)，有許多不同的人臉，包括男性女性老人嬰兒，我們取出其中的**8649**張圖片，全部放在Others資料夾，當作是一個新的類別，因此現在的**類別有六項，也就是五位女星加上一類其他人，總共有17223筆Data**。<br>\n",
    "\n",
    "這次我們就不再把17223筆Data切割Testing data，而是全部當作Training data，最後直接在影片上看效果，其實可以發現大致上成功的達到我們想要的答案，神經網路中的training accuracy 也都能達到0.993左右。<br>\n",
    "\n",
    "但是其實還有一些可以改進的部分，如果遇到臉太相近的女團，或是影片中表情動作太豐富(例如[這裡](https://www.youtube.com/watch?v=HYich6WOVhk))，在影片中的效果就不是很好，推測是因為影片中可能會有模糊，臉部角度，明暗度也比較多元的關係，如果能夠training set是影片，testing set也是影片，我相信我們能夠做出更好的結果。但是由於本來的設定是韓國女團人臉圖片辨識，就沒用影片下去做training data。例如"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這裡是結果呈現4的Training 過程\n",
    "比較需要注意的只有多一類Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = ['.\\images\\M','.\\images\\J','.\\images\\\\N','.\\images\\S','.\\images\\Z','.\\images\\Others']\n",
    "files = [[],[],[],[],[],[]]\n",
    "# r=root, d=directories, f = files\n",
    "for index in range(6):\n",
    "    for r, d, f in os.walk(path[index]):\n",
    "        for file in f:\n",
    "            files[index].append(os.path.join(r, file))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCount:17223\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n",
    "\n",
    "# 最後要記錄出的樣品向量(128 維度) 和標籤(明星名)\n",
    "encodinglist = []\n",
    "labels = []\n",
    "# 一橫列畫幾個人\n",
    "count=0\n",
    "for index_of_person in range(6):\n",
    "    # 總共幾個橫列\n",
    "    #height = int(len(files[index_of_person]) / width) + 1\n",
    "    # 整個大圖的size\n",
    "    #plt.figure(figsize=(100,100))\n",
    "    #print('------------')\n",
    "    for (i, f) in enumerate(files[index_of_person]):\n",
    "        #if i >600: \n",
    "        #if f == (\".\\images\\S\\SinB.full.160333.jpg\"): \n",
    "        # Step1. 讀取檔案\n",
    "        \n",
    "            if (index_of_person <5 ):\n",
    "                for k in range(3):\n",
    "                    if(k==0):\n",
    "                        img = face_recognition.load_image_file(f)\n",
    "                    elif(k==1):\n",
    "                        img = face_recognition.load_image_file(f)\n",
    "                        img = cv2.flip(img, 1)\n",
    "                    else:\n",
    "                        img = face_recognition.load_image_file(f)\n",
    "                        img = increase_brightness(img, value=80)\n",
    "\n",
    "                    # Step2. 把臉的降維向量算出, 用已經做好的cnn, [0] 第一張臉\n",
    "                    face_detect_and_encoding = face_recognition.face_encodings(img)\n",
    "                            #print(face_encoding[0])\n",
    "                        # Step3. 抓出臉的方框, 我們這裡沒用到, 但如果你想畫框就會需要\n",
    "                        # 這裡的face_locations 可以使用cnn(深度學習) 或者haar(固定特徵) 來得到\n",
    "                        # 如果需要快速, 請使用haar, 如果需要更準確, 就使用cnn, 建議可以使用預設haar 即可\n",
    "                    if len(face_detect_and_encoding) <= 0:\n",
    "                        #print(\"No faces found in the image!\")\n",
    "                        continue\n",
    "                    else :\n",
    "                        face_encoding = face_detect_and_encoding[0]\n",
    "\n",
    "                    #print(f+\", NO:\"+str(count))\n",
    "                    count=count+1\n",
    "\n",
    "                        # Step4. 把它加到我自己準備的list 裡\n",
    "                    encodinglist.append(face_encoding)\n",
    "                        # Step6. 把人名到我自己準備的list\n",
    "                    labels.append(index_of_person)\n",
    "            else:\n",
    "                img = face_recognition.load_image_file(f)\n",
    "                face_detect_and_encoding = face_recognition.face_encodings(img)\n",
    "                if len(face_detect_and_encoding) <= 0:\n",
    "                    #print(\"No faces found in the image!\")\n",
    "                    continue\n",
    "                else :\n",
    "                    face_encoding = face_detect_and_encoding[0]\n",
    "                    #print(f+\", NO:\"+str(count))\n",
    "                    count=count+1            \n",
    "\n",
    "                encodinglist.append(face_encoding)\n",
    "                labels.append(index_of_person)            \n",
    "\n",
    "            # 利用enumerate 得到的i 指定subplot\n",
    "                #plt.subplot(height, width, i + 1)\n",
    "                #plt.axis(\"off\")\n",
    "                #plt.imshow(img)\n",
    "                \n",
    "print(\"TotalCount:\"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam ,SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x長度: 17223\n",
      "y長度: 17223\n",
      "x長度: 17223\n"
     ]
    }
   ],
   "source": [
    "# 秀一下降維過後的向量\n",
    "print(\"x長度:\", len(encodinglist))\n",
    "print(\"y長度:\", len(labels))\n",
    "\n",
    "data_length = len(encodinglist)\n",
    "\n",
    "data_x = np.array(encodinglist)\n",
    "data_y = np.array(labels)\n",
    "\n",
    "x_train = data_x.reshape(data_length, 128, 1, 1)\n",
    "\n",
    "y_train = np_utils.to_categorical(data_y, 6)\n",
    "\n",
    "#x_train,x_test,y_train,y_test =train_test_split( data_x , data_y ,test_size = 0.15, random_state = 4)\n",
    "\n",
    "\n",
    "print(\"x長度:\", len(y_train))\n",
    "#print(\"y長度:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 1, 40)        200       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 1, 40)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 1, 40)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 1, 60)         9660      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 1, 60)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 1, 60)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 1, 80)         19280     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 1, 80)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 1, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 1, 100)        32100     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 1, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 906       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 182,296\n",
      "Trainable params: 182,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17223 samples, validate on 17223 samples\n",
      "Epoch 1/20\n",
      "17223/17223 [==============================] - 53s 3ms/step - loss: 0.0980 - acc: 0.9631 - val_loss: 0.0449 - val_acc: 0.9841\n",
      "Epoch 2/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0481 - acc: 0.9833 - val_loss: 0.0371 - val_acc: 0.9872\n",
      "Epoch 3/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0414 - acc: 0.9855 - val_loss: 0.0302 - val_acc: 0.9896\n",
      "Epoch 4/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0371 - acc: 0.9867 - val_loss: 0.0235 - val_acc: 0.9919\n",
      "Epoch 5/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0326 - acc: 0.9882 - val_loss: 0.0438 - val_acc: 0.9853\n",
      "Epoch 6/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0310 - acc: 0.9891 - val_loss: 0.0441 - val_acc: 0.9850\n",
      "Epoch 7/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0294 - acc: 0.9895 - val_loss: 0.0201 - val_acc: 0.9925\n",
      "Epoch 8/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0283 - acc: 0.9896 - val_loss: 0.0204 - val_acc: 0.9924\n",
      "Epoch 9/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0276 - acc: 0.9906 - val_loss: 0.0218 - val_acc: 0.9919\n",
      "Epoch 10/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0249 - acc: 0.9912 - val_loss: 0.0314 - val_acc: 0.9883\n",
      "Epoch 11/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0254 - acc: 0.9911 - val_loss: 0.0279 - val_acc: 0.9907\n",
      "Epoch 12/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0244 - acc: 0.9911 - val_loss: 0.0168 - val_acc: 0.9939\n",
      "Epoch 13/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0243 - acc: 0.9915 - val_loss: 0.0145 - val_acc: 0.9947\n",
      "Epoch 14/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0225 - acc: 0.9921 - val_loss: 0.0233 - val_acc: 0.9915\n",
      "Epoch 15/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0224 - acc: 0.9921 - val_loss: 0.0175 - val_acc: 0.9934\n",
      "Epoch 16/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.0177 - val_acc: 0.9933\n",
      "Epoch 17/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0219 - acc: 0.9925 - val_loss: 0.0213 - val_acc: 0.9935\n",
      "Epoch 18/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0150 - val_acc: 0.9946\n",
      "Epoch 19/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0202 - acc: 0.9928 - val_loss: 0.0139 - val_acc: 0.9951\n",
      "Epoch 20/20\n",
      "17223/17223 [==============================] - 52s 3ms/step - loss: 0.0206 - acc: 0.9929 - val_loss: 0.0134 - val_acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(40, (4,1), padding='same', input_shape=(128,1,1))) #4個filter，都是5*5\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model3.add(Conv2D(60, (4,1), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model3.add(Conv2D(80, (4,1), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model3.add(Conv2D(100, (4,1), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPool2D(pool_size=(2,1)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(150))#拉平完送進最後一個普通NN\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "model3.add(Dense(6))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "model3_out=model3.fit(x_train, y_train, batch_size=3, epochs=20,verbose =1,validation_data = (x_train,y_train))\n",
    "\n",
    "model3_json = model3.to_json()\n",
    "open('model3.json', 'w').write(model3_json)\n",
    "model3.save_weights('model3_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面可以看到Training data accuracy 在0.993，算是還不錯的成績\n",
    "\n",
    "### 這裡是結果呈現4的執行過程\n",
    "\n",
    "多人在同一個影片中的人臉偵測和辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "import face_recognition\n",
    "\n",
    "model3 = model_from_json(open('./model3.json').read())\n",
    "model3.load_weights('./model3_weights.h5')\n",
    "model3.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NameLabel(people):\n",
    "    if(people==0):\n",
    "        return \"Min-Joo\"\n",
    "    elif(people==1):\n",
    "        return \"Jessica\"\n",
    "    elif(people==2):\n",
    "        return \"Na-Gyung\"\n",
    "    elif(people==3):\n",
    "        return \"Sinb\"\n",
    "    elif(people==4):\n",
    "        return \"Tzu-Yu\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "    \n",
    "frame_no=0\n",
    "cv2.namedWindow('frame', cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "def ChangeFrame(value):\n",
    "    ;\n",
    "\n",
    "cap = cv2.VideoCapture('./images/Videos/M_people3.mp4')\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "color = (0, 266, 266)\n",
    "frame_lenth = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cv2.createTrackbar('Frame','frame',0,frame_lenth,ChangeFrame)\n",
    "cv2.createTrackbar('Auto','frame',0,1,ChangeFrame)\n",
    "while(cap.isOpened()):\n",
    "    #自動撥放\n",
    "    if(cv2.getTrackbarPos('Auto','frame')==1):\n",
    "        ;\n",
    "    #設定要哪個frame\n",
    "    else:\n",
    "        cap.set(1,cv2.getTrackbarPos('Frame','frame'))\n",
    "        \n",
    "    ret, frame = cap.read()\n",
    "    height,width = frame.shape[:2]\n",
    "    frame = cv2.resize(frame,(width//3,height//3),interpolation=cv2.INTER_CUBIC)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #=======================================predict face\n",
    "    # (top, right, bottom, left)(top, right, bottom, left) = \n",
    "    '''\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    #print(len(face_locations))\n",
    "    for i in range(len(face_locations)):\n",
    "        (top, right, bottom, left) =  face_locations[i]\n",
    "        # rectangle(要畫的圖, 左上座標, 右下座標, 顏色, 粗細)\n",
    "        size = int(frame.shape[0] / 100)\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), color, size)\n",
    "        #=========================perdict face\n",
    "     ''' \n",
    "    Testface_encoding = face_recognition.face_encodings(frame)\n",
    "    face_locations = face_recognition.face_locations(frame)  \n",
    "       #print(len(Testface_encoding))\n",
    "        #print(Testdata_x.shape[0])\n",
    "        #https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image\n",
    "    for People in range(len(Testface_encoding)):\n",
    "            \n",
    "            (top, right, bottom, left) =  face_locations[People]\n",
    "            size = int(frame.shape[0] / 100)\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), color, size)\n",
    "            \n",
    "            \n",
    "            Testdata_x = np.array(Testface_encoding[People])\n",
    "            #Threshold(Testdata_x)\n",
    "            Testdata_x = Testdata_x.reshape(1,128, 1, 1)#注意這裡最前面要加一個1\n",
    "            predict = model3.predict_classes(Testdata_x)\n",
    "            #predictvalue = model1.predict(Testdata_x)\n",
    "            #print(predictvalue)\n",
    "            #if(predictvalue[0][predict]>0.9999):\n",
    "            predictLabel = NameLabel(predict[0])\n",
    "            #print(predictLabel)\n",
    "            cv2.putText(frame, predictLabel, (left+20 , bottom+20 ),font,0.5,color)\n",
    "            #print(predictLabel)\n",
    "        #===========================\n",
    "    \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
