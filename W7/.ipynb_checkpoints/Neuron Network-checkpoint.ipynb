{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#整理input\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "#整理output 成 1-hot enconding\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.重現上課的 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 3140      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,180\n",
      "Trainable params: 3,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mode_class = Sequential()\n",
    "mode_class.add(Dense(4, input_dim=784))\n",
    "mode_class.add(Activation('sigmoid'))\n",
    "mode_class.add(Dense(2))\n",
    "mode_class.add(Activation('sigmoid'))\n",
    "mode_class.add(Dense(10))\n",
    "mode_class.add(Activation('softmax'))\n",
    "mode_class.compile(loss='mse', optimizer=SGD(lr=0.087), metrics=['accuracy'])\n",
    "mode_class.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0894 - acc: 0.1521 - val_loss: 0.0893 - val_acc: 0.1581\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0893 - acc: 0.1526 - val_loss: 0.0893 - val_acc: 0.1587\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1537 - val_loss: 0.0893 - val_acc: 0.1603\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1547 - val_loss: 0.0893 - val_acc: 0.1613\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1565 - val_loss: 0.0893 - val_acc: 0.1615\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0893 - acc: 0.1576 - val_loss: 0.0893 - val_acc: 0.1626\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0893 - acc: 0.1590 - val_loss: 0.0893 - val_acc: 0.1631\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1601 - val_loss: 0.0893 - val_acc: 0.1643\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0893 - acc: 0.1609 - val_loss: 0.0892 - val_acc: 0.1656\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0893 - acc: 0.1617 - val_loss: 0.0892 - val_acc: 0.1668\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0893 - acc: 0.1625 - val_loss: 0.0892 - val_acc: 0.1674\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0893 - acc: 0.1636 - val_loss: 0.0892 - val_acc: 0.1682\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0893 - acc: 0.1646 - val_loss: 0.0892 - val_acc: 0.1693\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1658 - val_loss: 0.0892 - val_acc: 0.1703\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0893 - acc: 0.1667 - val_loss: 0.0892 - val_acc: 0.1714\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.1678 - val_loss: 0.0892 - val_acc: 0.1717\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0892 - acc: 0.1684 - val_loss: 0.0892 - val_acc: 0.1716\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.1694 - val_loss: 0.0892 - val_acc: 0.1722\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.1702 - val_loss: 0.0892 - val_acc: 0.1723\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.1709 - val_loss: 0.0892 - val_acc: 0.1726\n"
     ]
    }
   ],
   "source": [
    "model_class_result=mode_class.fit(x_train, y_train, batch_size=100, epochs=20,verbose =1,validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把資料正規化\n",
    "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
    "x_test = (x_test - x_test.min())/(x_test.max() - x_test.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 調整layer與neuron 數量\n",
    "\n",
    "因為有點想嘗試 deep 的 NN ，所以先增加為10層，每層3~5個neuron試試看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 4)                 3140      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,340\n",
      "Trainable params: 3,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(4, input_dim=784))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(5))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(4))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(3))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(3))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(5))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(3))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(4))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(4))\n",
    "model_2.add(Activation(\"sigmoid\"))\n",
    "model_2.add(Dense(10))\n",
    "model_2.add(Activation(\"softmax\"))\n",
    "model_2.compile(loss='mse', optimizer=SGD(lr=0.008), metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0907 - acc: 0.0904 - val_loss: 0.0907 - val_acc: 0.0892\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0907 - acc: 0.0904 - val_loss: 0.0907 - val_acc: 0.0892\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0907 - acc: 0.0903 - val_loss: 0.0907 - val_acc: 0.0892\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0906 - acc: 0.0903 - val_loss: 0.0906 - val_acc: 0.0892\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0906 - acc: 0.0904 - val_loss: 0.0906 - val_acc: 0.0892\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0906 - acc: 0.0904 - val_loss: 0.0906 - val_acc: 0.0892\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0906 - acc: 0.0904 - val_loss: 0.0905 - val_acc: 0.0892\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0905 - acc: 0.0903 - val_loss: 0.0905 - val_acc: 0.0892\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0905 - acc: 0.0903 - val_loss: 0.0905 - val_acc: 0.0892\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0905 - acc: 0.0903 - val_loss: 0.0905 - val_acc: 0.0892\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0905 - acc: 0.0903 - val_loss: 0.0904 - val_acc: 0.0892\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0904 - acc: 0.0904 - val_loss: 0.0904 - val_acc: 0.0892\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0904 - acc: 0.0903 - val_loss: 0.0904 - val_acc: 0.0892\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0904 - acc: 0.0904 - val_loss: 0.0904 - val_acc: 0.0892\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0904 - acc: 0.0904 - val_loss: 0.0904 - val_acc: 0.0892\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0904 - acc: 0.0904 - val_loss: 0.0903 - val_acc: 0.0892\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0903 - acc: 0.0903 - val_loss: 0.0903 - val_acc: 0.0892\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0903 - acc: 0.0903 - val_loss: 0.0903 - val_acc: 0.0892\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0903 - acc: 0.0903 - val_loss: 0.0903 - val_acc: 0.0892\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0903 - acc: 0.0903 - val_loss: 0.0903 - val_acc: 0.0892\n"
     ]
    }
   ],
   "source": [
    "model_2_result = model_2.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 20,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面可以看到，增加層數好像未必能帶來提升，反而讓結果更不好了。\n",
    "\n",
    "### 3.增加每層的神經元數量\n",
    "\n",
    "這裡回到2層hidden layer，但是每層的神經元數量提高，並把learning rate也提高看看，也把學習的次數提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0900 - acc: 0.1264 - val_loss: 0.0893 - val_acc: 0.2317\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0890 - acc: 0.2251 - val_loss: 0.0886 - val_acc: 0.2446\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0882 - acc: 0.2564 - val_loss: 0.0876 - val_acc: 0.2506\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0870 - acc: 0.2542 - val_loss: 0.0862 - val_acc: 0.2558\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0852 - acc: 0.2515 - val_loss: 0.0838 - val_acc: 0.2330\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0821 - acc: 0.2728 - val_loss: 0.0799 - val_acc: 0.2961\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0778 - acc: 0.3573 - val_loss: 0.0751 - val_acc: 0.4401\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0732 - acc: 0.4541 - val_loss: 0.0705 - val_acc: 0.4978\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0687 - acc: 0.5136 - val_loss: 0.0658 - val_acc: 0.5542\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0638 - acc: 0.5777 - val_loss: 0.0605 - val_acc: 0.6322\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0584 - acc: 0.6314 - val_loss: 0.0549 - val_acc: 0.6608\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0528 - acc: 0.6729 - val_loss: 0.0493 - val_acc: 0.7026\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0475 - acc: 0.7153 - val_loss: 0.0442 - val_acc: 0.7577\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0427 - acc: 0.7606 - val_loss: 0.0398 - val_acc: 0.7907\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0386 - acc: 0.7941 - val_loss: 0.0360 - val_acc: 0.8163\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0351 - acc: 0.8170 - val_loss: 0.0327 - val_acc: 0.8332\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0321 - acc: 0.8328 - val_loss: 0.0300 - val_acc: 0.8464\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0296 - acc: 0.8448 - val_loss: 0.0276 - val_acc: 0.8559\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0274 - acc: 0.8544 - val_loss: 0.0257 - val_acc: 0.8629\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0256 - acc: 0.8610 - val_loss: 0.0240 - val_acc: 0.8708\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0242 - acc: 0.8669 - val_loss: 0.0227 - val_acc: 0.8752\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0229 - acc: 0.8719 - val_loss: 0.0216 - val_acc: 0.8787\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0219 - acc: 0.8751 - val_loss: 0.0206 - val_acc: 0.8843\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0210 - acc: 0.8789 - val_loss: 0.0198 - val_acc: 0.8867\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0203 - acc: 0.8812 - val_loss: 0.0191 - val_acc: 0.8893\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0196 - acc: 0.8839 - val_loss: 0.0185 - val_acc: 0.8922\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0191 - acc: 0.8863 - val_loss: 0.0180 - val_acc: 0.8938\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0186 - acc: 0.8887 - val_loss: 0.0175 - val_acc: 0.8947\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0182 - acc: 0.8907 - val_loss: 0.0171 - val_acc: 0.8955\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0178 - acc: 0.8923 - val_loss: 0.0168 - val_acc: 0.8975\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0174 - acc: 0.8933 - val_loss: 0.0164 - val_acc: 0.8985\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0171 - acc: 0.8949 - val_loss: 0.0161 - val_acc: 0.9000\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0168 - acc: 0.8963 - val_loss: 0.0159 - val_acc: 0.9008\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0166 - acc: 0.8974 - val_loss: 0.0157 - val_acc: 0.9014\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0163 - acc: 0.8985 - val_loss: 0.0154 - val_acc: 0.9037\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0161 - acc: 0.8993 - val_loss: 0.0152 - val_acc: 0.9029\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0159 - acc: 0.9007 - val_loss: 0.0150 - val_acc: 0.9044\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0157 - acc: 0.9017 - val_loss: 0.0149 - val_acc: 0.9054\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0155 - acc: 0.9025 - val_loss: 0.0147 - val_acc: 0.9060\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0153 - acc: 0.9038 - val_loss: 0.0145 - val_acc: 0.9075\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(200, input_dim=784))\n",
    "model_3.add(Activation(\"sigmoid\"))\n",
    "model_3.add(Dense(100))\n",
    "model_3.add(Activation(\"sigmoid\"))\n",
    "model_3.add(Dense(10))\n",
    "model_3.add(Activation(\"softmax\"))\n",
    "model_3.compile(loss='mse', optimizer=SGD(lr=0.2), metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "\n",
    "model_3_result = model_3.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以2層hidden layer，每層的neuron增加，訓練40個週期後已經突破九成了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_class_result.history[\"val_acc\"])\n",
    "plt.plot(model_2_result.history[\"val_acc\"])\n",
    "plt.plot(model_3_result.history[\"val_acc\"])\n",
    "\n",
    "plt.title(\"testing data accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"Class model\",\"deep model\",\"wide model\"],loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 修改 activation function\n",
    "\n",
    "上課時助教還是老師好像有建議可以用Relu看看，因此這裡就用先試用這個activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0609 - acc: 0.5667 - val_loss: 0.0291 - val_acc: 0.8458\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0224 - acc: 0.8704 - val_loss: 0.0172 - val_acc: 0.8970\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0166 - acc: 0.8974 - val_loss: 0.0143 - val_acc: 0.9112\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0144 - acc: 0.9093 - val_loss: 0.0130 - val_acc: 0.9172\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0131 - acc: 0.9170 - val_loss: 0.0120 - val_acc: 0.9241\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0122 - acc: 0.9235 - val_loss: 0.0115 - val_acc: 0.9260\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0115 - acc: 0.9277 - val_loss: 0.0108 - val_acc: 0.9306\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0108 - acc: 0.9321 - val_loss: 0.0104 - val_acc: 0.9347\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0103 - acc: 0.9358 - val_loss: 0.0098 - val_acc: 0.9384\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0098 - acc: 0.9386 - val_loss: 0.0094 - val_acc: 0.9397\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0093 - acc: 0.9419 - val_loss: 0.0092 - val_acc: 0.9418\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0089 - acc: 0.9445 - val_loss: 0.0088 - val_acc: 0.9444\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.0086 - acc: 0.9471 - val_loss: 0.0085 - val_acc: 0.9460\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0082 - acc: 0.9487 - val_loss: 0.0083 - val_acc: 0.9465\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0079 - acc: 0.9511 - val_loss: 0.0080 - val_acc: 0.9486\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0076 - acc: 0.9530 - val_loss: 0.0078 - val_acc: 0.9494\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0074 - acc: 0.9550 - val_loss: 0.0075 - val_acc: 0.9511\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0071 - acc: 0.9562 - val_loss: 0.0073 - val_acc: 0.9525\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0069 - acc: 0.9580 - val_loss: 0.0072 - val_acc: 0.9541\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0066 - acc: 0.9592 - val_loss: 0.0070 - val_acc: 0.9550\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0064 - acc: 0.9610 - val_loss: 0.0069 - val_acc: 0.9560\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0062 - acc: 0.9620 - val_loss: 0.0067 - val_acc: 0.9557\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0061 - acc: 0.9635 - val_loss: 0.0066 - val_acc: 0.9576\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0059 - acc: 0.9649 - val_loss: 0.0064 - val_acc: 0.9587\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0057 - acc: 0.9659 - val_loss: 0.0063 - val_acc: 0.9596\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0056 - acc: 0.9667 - val_loss: 0.0061 - val_acc: 0.9602\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0054 - acc: 0.9679 - val_loss: 0.0060 - val_acc: 0.9619\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0053 - acc: 0.9689 - val_loss: 0.0060 - val_acc: 0.9612\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0051 - acc: 0.9697 - val_loss: 0.0059 - val_acc: 0.9624\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0050 - acc: 0.9711 - val_loss: 0.0058 - val_acc: 0.9622\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0049 - acc: 0.9714 - val_loss: 0.0057 - val_acc: 0.9640\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0047 - acc: 0.9722 - val_loss: 0.0055 - val_acc: 0.9648\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0046 - acc: 0.9732 - val_loss: 0.0055 - val_acc: 0.9642\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0045 - acc: 0.9742 - val_loss: 0.0054 - val_acc: 0.9654\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0044 - acc: 0.9749 - val_loss: 0.0054 - val_acc: 0.9648\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0043 - acc: 0.9756 - val_loss: 0.0053 - val_acc: 0.9671\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0042 - acc: 0.9762 - val_loss: 0.0052 - val_acc: 0.9670\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0041 - acc: 0.9768 - val_loss: 0.0051 - val_acc: 0.9678\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0040 - acc: 0.9771 - val_loss: 0.0051 - val_acc: 0.9681\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0039 - acc: 0.9778 - val_loss: 0.0050 - val_acc: 0.9687\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(200, input_dim=784))\n",
    "model_3.add(Activation(\"relu\"))\n",
    "model_3.add(Dense(100))\n",
    "model_3.add(Activation(\"relu\"))\n",
    "model_3.add(Dense(10))\n",
    "model_3.add(Activation(\"softmax\"))\n",
    "model_3.compile(loss='mse', optimizer=SGD(lr=0.2), metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "\n",
    "model_3_result = model_3.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正確率大約已經來到了97 %，下面試試看用selu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_154 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_154 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_155 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0301 - acc: 0.8045 - val_loss: 0.0168 - val_acc: 0.8960\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0160 - acc: 0.8982 - val_loss: 0.0140 - val_acc: 0.9116\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0140 - acc: 0.9102 - val_loss: 0.0129 - val_acc: 0.9163\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0130 - acc: 0.9165 - val_loss: 0.0122 - val_acc: 0.9227\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0123 - acc: 0.9209 - val_loss: 0.0117 - val_acc: 0.9253\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0117 - acc: 0.9251 - val_loss: 0.0112 - val_acc: 0.9292\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0113 - acc: 0.9279 - val_loss: 0.0108 - val_acc: 0.9313\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0109 - acc: 0.9312 - val_loss: 0.0106 - val_acc: 0.9319\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0105 - acc: 0.9331 - val_loss: 0.0104 - val_acc: 0.9327\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0102 - acc: 0.9353 - val_loss: 0.0100 - val_acc: 0.9359\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0099 - acc: 0.9376 - val_loss: 0.0100 - val_acc: 0.9354\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0096 - acc: 0.9396 - val_loss: 0.0096 - val_acc: 0.9383\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0093 - acc: 0.9413 - val_loss: 0.0093 - val_acc: 0.9397\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0090 - acc: 0.9433 - val_loss: 0.0092 - val_acc: 0.9397\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0088 - acc: 0.9453 - val_loss: 0.0090 - val_acc: 0.9422\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0086 - acc: 0.9466 - val_loss: 0.0088 - val_acc: 0.9427\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0083 - acc: 0.9481 - val_loss: 0.0086 - val_acc: 0.9439\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0081 - acc: 0.9497 - val_loss: 0.0085 - val_acc: 0.9447\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0079 - acc: 0.9512 - val_loss: 0.0083 - val_acc: 0.9459\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0077 - acc: 0.9522 - val_loss: 0.0082 - val_acc: 0.9469\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0075 - acc: 0.9538 - val_loss: 0.0080 - val_acc: 0.9477\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0073 - acc: 0.9551 - val_loss: 0.0079 - val_acc: 0.9476\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0071 - acc: 0.9566 - val_loss: 0.0077 - val_acc: 0.9490\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0069 - acc: 0.9578 - val_loss: 0.0076 - val_acc: 0.9512\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0068 - acc: 0.9588 - val_loss: 0.0074 - val_acc: 0.9515\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0066 - acc: 0.9599 - val_loss: 0.0073 - val_acc: 0.9536\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0065 - acc: 0.9605 - val_loss: 0.0071 - val_acc: 0.9537\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0063 - acc: 0.9618 - val_loss: 0.0070 - val_acc: 0.9545\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0062 - acc: 0.9629 - val_loss: 0.0069 - val_acc: 0.9563\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0060 - acc: 0.9640 - val_loss: 0.0068 - val_acc: 0.9556\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0059 - acc: 0.9644 - val_loss: 0.0066 - val_acc: 0.9562\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0058 - acc: 0.9652 - val_loss: 0.0065 - val_acc: 0.9573\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0056 - acc: 0.9664 - val_loss: 0.0065 - val_acc: 0.9573\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0055 - acc: 0.9670 - val_loss: 0.0064 - val_acc: 0.9574\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0054 - acc: 0.9680 - val_loss: 0.0063 - val_acc: 0.9588\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0053 - acc: 0.9690 - val_loss: 0.0062 - val_acc: 0.9591\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0052 - acc: 0.9695 - val_loss: 0.0062 - val_acc: 0.9608\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0051 - acc: 0.9706 - val_loss: 0.0060 - val_acc: 0.9609\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0050 - acc: 0.9710 - val_loss: 0.0060 - val_acc: 0.9609\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0049 - acc: 0.9713 - val_loss: 0.0059 - val_acc: 0.9614\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Dense(200, input_dim=784))\n",
    "model_4.add(Activation(\"selu\"))\n",
    "model_4.add(Dense(100))\n",
    "model_4.add(Activation(\"selu\"))\n",
    "model_4.add(Dense(10))\n",
    "model_4.add(Activation(\"softmax\"))\n",
    "model_4.compile(loss='mse', optimizer=SGD(lr=0.2), metrics=['accuracy'])\n",
    "model_4.summary()\n",
    "\n",
    "model_4_result = model_4.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "發現效果其實沒有差太多，relu還是高個0.0幾，雖然可能只是數據誤差，但我還是用回relu。\n",
    "\n",
    "### 5. 修改Optimiter\n",
    "這裡先用adam試試看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_127 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1803 - acc: 0.0977 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.1805 - acc: 0.0975 - val_loss: 0.1805 - val_acc: 0.0974\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "model_5 = Sequential()\n",
    "model_5.add(Dense(200, input_dim=784))\n",
    "model_5.add(Activation(\"relu\"))\n",
    "model_5.add(Dense(100))\n",
    "model_5.add(Activation(\"relu\"))\n",
    "model_5.add(Dense(10))\n",
    "model_5.add(Activation(\"softmax\"))\n",
    "model_5.compile(loss='mse', optimizer=Adam(lr=0.2), metrics=['accuracy'])\n",
    "model_5.summary()\n",
    "\n",
    "model_5_result = model_5.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 20,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到在test data和training data上表現得都超不好，甚至比上課的model還差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_148 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0608 - acc: 0.5763 - val_loss: 0.0300 - val_acc: 0.8405\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0228 - acc: 0.8694 - val_loss: 0.0174 - val_acc: 0.8973\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0166 - acc: 0.8987 - val_loss: 0.0144 - val_acc: 0.9106\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0144 - acc: 0.9101 - val_loss: 0.0130 - val_acc: 0.9180\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0131 - acc: 0.9173 - val_loss: 0.0120 - val_acc: 0.9253\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0122 - acc: 0.9234 - val_loss: 0.0113 - val_acc: 0.9303\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0114 - acc: 0.9286 - val_loss: 0.0109 - val_acc: 0.9320\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0108 - acc: 0.9324 - val_loss: 0.0103 - val_acc: 0.9357\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0103 - acc: 0.9363 - val_loss: 0.0098 - val_acc: 0.9389\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0098 - acc: 0.9389 - val_loss: 0.0094 - val_acc: 0.9409\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0094 - acc: 0.9414 - val_loss: 0.0091 - val_acc: 0.9423\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0090 - acc: 0.9439 - val_loss: 0.0089 - val_acc: 0.9448\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0087 - acc: 0.9459 - val_loss: 0.0086 - val_acc: 0.9455\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0084 - acc: 0.9480 - val_loss: 0.0083 - val_acc: 0.9474\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0081 - acc: 0.9500 - val_loss: 0.0081 - val_acc: 0.9492\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0078 - acc: 0.9523 - val_loss: 0.0081 - val_acc: 0.9493\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0075 - acc: 0.9535 - val_loss: 0.0077 - val_acc: 0.9504\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0073 - acc: 0.9555 - val_loss: 0.0075 - val_acc: 0.9529\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0071 - acc: 0.9568 - val_loss: 0.0073 - val_acc: 0.9532\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0068 - acc: 0.9582 - val_loss: 0.0071 - val_acc: 0.9555\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0066 - acc: 0.9596 - val_loss: 0.0070 - val_acc: 0.9550\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0064 - acc: 0.9614 - val_loss: 0.0069 - val_acc: 0.9547\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0062 - acc: 0.9621 - val_loss: 0.0067 - val_acc: 0.9569\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0061 - acc: 0.9634 - val_loss: 0.0065 - val_acc: 0.9585\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0059 - acc: 0.9651 - val_loss: 0.0063 - val_acc: 0.9600\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0057 - acc: 0.9660 - val_loss: 0.0063 - val_acc: 0.9590\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0056 - acc: 0.9671 - val_loss: 0.0062 - val_acc: 0.9604\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0054 - acc: 0.9676 - val_loss: 0.0060 - val_acc: 0.9625\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0053 - acc: 0.9687 - val_loss: 0.0059 - val_acc: 0.9621\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0052 - acc: 0.9696 - val_loss: 0.0058 - val_acc: 0.9626\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0050 - acc: 0.9707 - val_loss: 0.0058 - val_acc: 0.9626\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0049 - acc: 0.9717 - val_loss: 0.0056 - val_acc: 0.9636\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0048 - acc: 0.9722 - val_loss: 0.0056 - val_acc: 0.9637\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0047 - acc: 0.9727 - val_loss: 0.0055 - val_acc: 0.9640\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0046 - acc: 0.9735 - val_loss: 0.0054 - val_acc: 0.9657\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0045 - acc: 0.9744 - val_loss: 0.0053 - val_acc: 0.9670\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0044 - acc: 0.9753 - val_loss: 0.0052 - val_acc: 0.9668\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0043 - acc: 0.9756 - val_loss: 0.0052 - val_acc: 0.9665\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0042 - acc: 0.9762 - val_loss: 0.0051 - val_acc: 0.9668\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0041 - acc: 0.9766 - val_loss: 0.0050 - val_acc: 0.9691\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adadelta\n",
    "model_6 = Sequential()\n",
    "model_6.add(Dense(200, input_dim=784))\n",
    "model_6.add(Activation(\"relu\"))\n",
    "model_6.add(Dense(100))\n",
    "model_6.add(Activation(\"relu\"))\n",
    "model_6.add(Dense(10))\n",
    "model_6.add(Activation(\"softmax\"))\n",
    "model_6.compile(loss='mse', optimizer=Adadelta(lr=0.2), metrics=['accuracy'])\n",
    "model_6.summary()\n",
    "\n",
    "model_6_result = model_6.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後把助教上課有寫的Optimizer都試了一遍，發現除了Adadelta之外其他都和Adam一樣正確率掉到很低，因此就選用這個optimiter\n",
    "\n",
    "### 6. 修改 Loss function\n",
    "\n",
    "最後來修改看看Loss function ， 第一個選用 binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_151 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_153 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.1155 - acc: 0.9597 - val_loss: 0.0570 - val_acc: 0.9816\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0530 - acc: 0.9828 - val_loss: 0.0455 - val_acc: 0.9853\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0440 - acc: 0.9857 - val_loss: 0.0392 - val_acc: 0.9873\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0384 - acc: 0.9877 - val_loss: 0.0349 - val_acc: 0.9888\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0342 - acc: 0.9890 - val_loss: 0.0315 - val_acc: 0.9901\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0308 - acc: 0.9901 - val_loss: 0.0292 - val_acc: 0.9905\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0268 - val_acc: 0.9914\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0258 - acc: 0.9917 - val_loss: 0.0249 - val_acc: 0.9921\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0238 - acc: 0.9925 - val_loss: 0.0234 - val_acc: 0.9924\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0219 - val_acc: 0.9929\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0208 - val_acc: 0.9932\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0194 - acc: 0.9939 - val_loss: 0.0204 - val_acc: 0.9931\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0197 - val_acc: 0.9935\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0171 - acc: 0.9947 - val_loss: 0.0192 - val_acc: 0.9937\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0176 - val_acc: 0.9941\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0171 - val_acc: 0.9943\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0145 - acc: 0.9955 - val_loss: 0.0169 - val_acc: 0.9943\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.0162 - val_acc: 0.9944\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0130 - acc: 0.9961 - val_loss: 0.0157 - val_acc: 0.9945\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0124 - acc: 0.9963 - val_loss: 0.0154 - val_acc: 0.9948\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.0150 - val_acc: 0.9947\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0113 - acc: 0.9966 - val_loss: 0.0149 - val_acc: 0.9948\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0145 - val_acc: 0.9948\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 0.0146 - val_acc: 0.9949\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0098 - acc: 0.9971 - val_loss: 0.0141 - val_acc: 0.9950\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0094 - acc: 0.9972 - val_loss: 0.0140 - val_acc: 0.9951\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0137 - val_acc: 0.9954\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0135 - val_acc: 0.9954\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0083 - acc: 0.9976 - val_loss: 0.0134 - val_acc: 0.9954\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.0133 - val_acc: 0.9954\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.0134 - val_acc: 0.9955\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0131 - val_acc: 0.9954\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0129 - val_acc: 0.9957\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0129 - val_acc: 0.9956\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0128 - val_acc: 0.9955\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 0.0125 - val_acc: 0.9958\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0126 - val_acc: 0.9957\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0058 - acc: 0.9985 - val_loss: 0.0125 - val_acc: 0.9956\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0122 - val_acc: 0.9957\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0120 - val_acc: 0.9958\n"
     ]
    }
   ],
   "source": [
    "model_7 = Sequential()\n",
    "model_7.add(Dense(200, input_dim=784))\n",
    "model_7.add(Activation(\"relu\"))\n",
    "model_7.add(Dense(100))\n",
    "model_7.add(Activation(\"relu\"))\n",
    "model_7.add(Dense(10))\n",
    "model_7.add(Activation(\"softmax\"))\n",
    "model_7.compile(loss='binary_crossentropy', optimizer=Adadelta(lr=0.2), metrics=['accuracy'])\n",
    "model_7.summary()\n",
    "\n",
    "model_7_result = model_7.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "發現已經達到了在testing data 大約0.996的程度，training data則是大約提升到0.999了 但我們還是照慣例試試看第二個Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_181 (Dense)            (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "activation_181 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_182 (Activation)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.9582 - acc: 0.4596 - val_loss: 0.9307 - val_acc: 0.7478\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.9191 - acc: 0.8460 - val_loss: 0.9128 - val_acc: 0.8875\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9119 - acc: 0.8883 - val_loss: 0.9101 - val_acc: 0.9011\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.9100 - acc: 0.9004 - val_loss: 0.9089 - val_acc: 0.9093\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9091 - acc: 0.9072 - val_loss: 0.9082 - val_acc: 0.9153\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.9084 - acc: 0.9129 - val_loss: 0.9077 - val_acc: 0.9187\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.9079 - acc: 0.9167 - val_loss: 0.9073 - val_acc: 0.9227\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.9075 - acc: 0.9203 - val_loss: 0.9070 - val_acc: 0.9256\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9072 - acc: 0.9237 - val_loss: 0.9068 - val_acc: 0.9268\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9069 - acc: 0.9265 - val_loss: 0.9065 - val_acc: 0.9288\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9067 - acc: 0.9293 - val_loss: 0.9063 - val_acc: 0.9303\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9065 - acc: 0.9315 - val_loss: 0.9061 - val_acc: 0.9321\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9063 - acc: 0.9336 - val_loss: 0.9060 - val_acc: 0.9340\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9061 - acc: 0.9352 - val_loss: 0.9059 - val_acc: 0.9358\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.9059 - acc: 0.9371 - val_loss: 0.9057 - val_acc: 0.9361\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.9057 - acc: 0.9389 - val_loss: 0.9056 - val_acc: 0.9381\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.9056 - acc: 0.9404 - val_loss: 0.9055 - val_acc: 0.9395\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.9055 - acc: 0.9415 - val_loss: 0.9054 - val_acc: 0.9407\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.9053 - acc: 0.9428 - val_loss: 0.9053 - val_acc: 0.9409\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.9052 - acc: 0.9441 - val_loss: 0.9052 - val_acc: 0.9427\n"
     ]
    }
   ],
   "source": [
    "model_8 = Sequential()\n",
    "model_8.add(Dense(200, input_dim=784))\n",
    "model_8.add(Activation(\"relu\"))\n",
    "model_8.add(Dense(100))\n",
    "model_8.add(Activation(\"relu\"))\n",
    "model_8.add(Dense(10))\n",
    "model_8.add(Activation(\"softmax\"))\n",
    "model_8.compile(loss='squared_hinge', optimizer=Adadelta(lr=0.2), metrics=['accuracy'])\n",
    "model_8.summary()\n",
    "\n",
    "model_8_result = model_8.fit(x_train, y_train,\n",
    "                        batch_size = 100,\n",
    "                        epochs = 20,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試了許多的loss function後都沒有超過binary_crossentropy，因此我想就保留前一個model。\n",
    "\n",
    "### 8.最後調整一下learning rate、neuron數量、batch size 等等的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_187 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "activation_187 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "activation_188 (Activation)  (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_189 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 297,710\n",
      "Trainable params: 297,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0676 - acc: 0.9769 - val_loss: 0.0397 - val_acc: 0.9871\n",
      "Epoch 2/40\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0332 - acc: 0.9892 - val_loss: 0.0343 - val_acc: 0.9878\n",
      "Epoch 3/40\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0244 - acc: 0.9922 - val_loss: 0.0230 - val_acc: 0.9924\n",
      "Epoch 4/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0193 - acc: 0.9939 - val_loss: 0.0186 - val_acc: 0.9937\n",
      "Epoch 5/40\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0175 - val_acc: 0.9941\n",
      "Epoch 6/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0134 - acc: 0.9958 - val_loss: 0.0155 - val_acc: 0.9948\n",
      "Epoch 7/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0145 - val_acc: 0.9949\n",
      "Epoch 8/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0139 - val_acc: 0.9953\n",
      "Epoch 9/40\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0085 - acc: 0.9974 - val_loss: 0.0131 - val_acc: 0.9956\n",
      "Epoch 10/40\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0132 - val_acc: 0.9955\n",
      "Epoch 11/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0131 - val_acc: 0.9955\n",
      "Epoch 12/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0121 - val_acc: 0.9958\n",
      "Epoch 13/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0123 - val_acc: 0.9957\n",
      "Epoch 14/40\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0117 - val_acc: 0.9959\n",
      "Epoch 15/40\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0118 - val_acc: 0.9962\n",
      "Epoch 16/40\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0124 - val_acc: 0.9958\n",
      "Epoch 17/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0118 - val_acc: 0.9961\n",
      "Epoch 18/40\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0116 - val_acc: 0.9962\n",
      "Epoch 19/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0114 - val_acc: 0.9964\n",
      "Epoch 20/40\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0124 - val_acc: 0.9961\n",
      "Epoch 21/40\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0116 - val_acc: 0.9963\n",
      "Epoch 22/40\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0114 - val_acc: 0.9965\n",
      "Epoch 23/40\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0120 - val_acc: 0.9964\n",
      "Epoch 24/40\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0117 - val_acc: 0.9963\n",
      "Epoch 25/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0116 - val_acc: 0.9963\n",
      "Epoch 26/40\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 9.4627e-04 - acc: 0.9999 - val_loss: 0.0117 - val_acc: 0.9965\n",
      "Epoch 27/40\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 8.2724e-04 - acc: 0.9999 - val_loss: 0.0119 - val_acc: 0.9965\n",
      "Epoch 28/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 7.4218e-04 - acc: 0.9999 - val_loss: 0.0120 - val_acc: 0.9964\n",
      "Epoch 29/40\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 6.8160e-04 - acc: 0.9999 - val_loss: 0.0124 - val_acc: 0.9965\n",
      "Epoch 30/40\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 6.0639e-04 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 0.9965\n",
      "Epoch 31/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 5.6625e-04 - acc: 0.9999 - val_loss: 0.0126 - val_acc: 0.9963\n",
      "Epoch 32/40\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 4.8557e-04 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9964\n",
      "Epoch 33/40\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 4.5212e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9963\n",
      "Epoch 34/40\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 4.0729e-04 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9964\n",
      "Epoch 35/40\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 3.8028e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9965\n",
      "Epoch 36/40\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 3.4676e-04 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 0.9964\n",
      "Epoch 37/40\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 3.3111e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9963\n",
      "Epoch 38/40\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 3.0253e-04 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9964\n",
      "Epoch 39/40\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 2.7757e-04 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9964\n",
      "Epoch 40/40\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 2.6823e-04 - acc: 1.0000 - val_loss: 0.0131 - val_acc: 0.9964\n"
     ]
    }
   ],
   "source": [
    "model_9 = Sequential()\n",
    "model_9.add(Dense(300, input_dim=784))\n",
    "model_9.add(Activation(\"relu\"))\n",
    "model_9.add(Dense(200))\n",
    "model_9.add(Activation(\"relu\"))\n",
    "model_9.add(Dense(10))\n",
    "model_9.add(Activation(\"softmax\"))\n",
    "model_9.compile(loss='binary_crossentropy', optimizer=Adadelta(lr=0.5), metrics=['accuracy'])\n",
    "model_9.summary()\n",
    "\n",
    "model_9_result = model_9.fit(x_train, y_train,\n",
    "                        batch_size = 70,\n",
    "                        epochs = 40,\n",
    "                        verbose = 1,\n",
    "                        validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\"> **最後的training data 正確率100%，但是testing data也就停留在99.65%左右了，如果我沒漏掉甚麼觀念，可能就要往資料preprocessing或是改成CNN或RNN model去試驗才能進一步提升了**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 列出所有試驗的model正確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 45us/step\n",
      "10000/10000 [==============================] - 0s 43us/step\n",
      "10000/10000 [==============================] - 1s 57us/step\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "10000/10000 [==============================] - 1s 56us/step\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "10000/10000 [==============================] - 1s 59us/step\n",
      "loss: 0.08917501438856125 正確率: 0.1726\n",
      "loss: 0.09028370124101638 正確率: 0.0892\n",
      "loss: 0.004679295310137968 正確率: 0.9697\n",
      "loss: 0.005888146921718726 正確率: 0.9614\n",
      "loss: 0.1805200044631958 正確率: 0.0974\n",
      "loss: 0.005043365660286509 正確率: 0.9691\n",
      "loss: 0.012463112213489058 正確率: 0.9963499971389771\n",
      "loss: 0.9051688905715942 正確率: 0.9427\n",
      "loss: 0.0131043899620076 正確率: 0.9964099969863892\n"
     ]
    }
   ],
   "source": [
    "score_class=mode_class.evaluate(x_test,y_test)\n",
    "score_2=model_2.evaluate(x_test,y_test)\n",
    "score_3=model_3.evaluate(x_test,y_test)\n",
    "score_4=model_4.evaluate(x_test,y_test)\n",
    "score_5=model_5.evaluate(x_test,y_test)\n",
    "score_6=model_6.evaluate(x_test,y_test)\n",
    "score_7=model_7.evaluate(x_test,y_test)\n",
    "score_8=model_8.evaluate(x_test,y_test)\n",
    "score_9=model_9.evaluate(x_test,y_test)\n",
    "print('loss:',score_class[0],'正確率:',score_class[1])\n",
    "print('loss:',score_2[0],'正確率:',score_2[1])\n",
    "print('loss:',score_3[0],'正確率:',score_3[1])\n",
    "print('loss:',score_4[0],'正確率:',score_4[1])\n",
    "print('loss:',score_5[0],'正確率:',score_5[1])\n",
    "print('loss:',score_6[0],'正確率:',score_6[1])\n",
    "print('loss:',score_7[0],'正確率:',score_7[1])\n",
    "print('loss:',score_8[0],'正確率:',score_8[1])\n",
    "print('loss:',score_9[0],'正確率:',score_9[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
