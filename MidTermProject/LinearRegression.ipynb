{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 此檔案定義 Linear Regression 相關 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddpolynomialFeature(data,degree):\n",
    "    for i in range(2,degree+1):\n",
    "        data['Sex'+str(i)] = data['Sex']**i\n",
    "        data['Length'+str(i)] = data['Length']**i\n",
    "        data['Diameter'+str(i)] = data['Diameter']**i\n",
    "        data['Height'+str(i)] = data['Height']**i\n",
    "        data['Whole weight'+str(i)] = data['Whole weight']**i\n",
    "        data['Shucked weight'+str(i)] = data['Shucked weight']**i\n",
    "        data['Viscera weight'+str(i)] = data['Viscera weight']**i\n",
    "        data['Shell weight'+str(i)] = data['Shell weight']**i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def LinearR_CallFunc_Auto(x_train,x_test,y_train,y_test)\n",
    "    poly = PolynomialFeatures(degree = 3,interaction_only=True) \n",
    "    X_train_poly = poly.fit_transform(x_train) \n",
    "    poly.fit(X_train_poly, y_train) \n",
    "    model = LinearRegression() \n",
    "    model.fit(X_train_poly, y_train) \n",
    "    \n",
    "    X_test_poly = poly.fit_transform(x_test) \n",
    "    Y_pred = model.predict(X_test_poly)\n",
    "    print('Call Library得到的R^2分數: '+str(model.score(X_test_poly,y_test)))\n",
    "'''\n",
    "#這裡是直接呼叫 library 的linear regression\n",
    "def LinearR_CallFunc(x_train,x_test,y_train,y_test): \n",
    "    model = LinearRegression() \n",
    "    model.fit(x_train,y_train)\n",
    "    Y_pred = model.predict(x_test)\n",
    "    print('Call Library得到的R^2分數: '+str(model.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從頭開始做 linear regression\n",
    "\n",
    "- cost function (或稱 loss function)式子:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CostFunction(θ,x,y): #Cost function\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    assert(θ.shape[0]==x.shape[1])#先確定他們的dim一樣\n",
    "    h = x @ θ #這是我們的hypothesis 也就是 θ0X0+θ1X1......θnXn = y 用 matmul，矩陣乘法一次算出全部\n",
    "    return(float(sum(np.multiply((h-y),(h-y)))/(2*m))) #multiply是elementwise 的乘法 (或用float((((h-y).T @ (h-y))/(2*m))))\n",
    "\n",
    "def GradientDescent(x,y,θ,α,num_iters):\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    J = np.zeros((num_iters,1))#我們把每一次的J都存起來，因為要是gradient decent是成功的，每輪的J應該會逐漸下降，不會上升\n",
    "    for iter in range(1,num_iters):#做num_iters次更新\n",
    "        delta = x.T @ ( ( x @ θ ) - y )   /m \n",
    "        θ = θ - α * delta\n",
    "        #print(CostFunction(θ,x,y))\n",
    "        J[iter][0]=CostFunction(θ,x,y)\n",
    "    return (θ , J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def NormalEqn(x,y):\n",
    "    return ( inv(x.T @ x) @ x.T @ y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畫出收斂圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the convergence of the cost function\n",
    "def plotConvergence(jvec,iterations):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for iter in range(1,iterations,100):\n",
    "        plt.plot(iter,jvec[iter][0],'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Cost function\")\n",
    "    \n",
    "def plotConvergence2(jvec,iterations):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for iter in range(1,iterations,1):\n",
    "        plt.plot(iter,jvec[iter][0],'bo')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Convergence of Cost Function\")\n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Cost function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef CostFunction(θ,x,y): #Cost function\\n    m = x.shape[0] # training set 共有幾組\\n    assert(θ.shape[0]==x.shape[1])#先確定他們的dim一樣\\n    h = np.matmul(x,θ) #這是我們的hypothesis 也就是 θ0X0+θ1X1......θnXn = y 用矩陣乘法一次算出全部\\n    return float(np.multiply((h-y),(h-y))/(2*m)) #multiply是elementwise 的乘法\\n\\n\\ndef GradientDescent(x,y,θ,α,num_iters):\\n    m = x.shape[0] # training set 共有幾組\\n    J = np.zeros((num_iters,1))#我們把每一次的J都存起來，因為要是gradient decent是成功的，每輪的J應該會逐漸下降，不會上升\\n    for iter in range(1,num_iters):#做num_iters次更新\\n        delta = (X.T @ ((X @ θ)-y))/m  #這是微分後的J\\n        θ = θ -α*delta\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def CostFunction(θ,x,y): #Cost function\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    assert(θ.shape[0]==x.shape[1])#先確定他們的dim一樣\n",
    "    h = np.matmul(x,θ) #這是我們的hypothesis 也就是 θ0X0+θ1X1......θnXn = y 用矩陣乘法一次算出全部\n",
    "    return float(np.multiply((h-y),(h-y))/(2*m)) #multiply是elementwise 的乘法\n",
    "\n",
    "\n",
    "def GradientDescent(x,y,θ,α,num_iters):\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    J = np.zeros((num_iters,1))#我們把每一次的J都存起來，因為要是gradient decent是成功的，每輪的J應該會逐漸下降，不會上升\n",
    "    for iter in range(1,num_iters):#做num_iters次更新\n",
    "        delta = (X.T @ ((X @ θ)-y))/m  #這是微分後的J\n",
    "        θ = θ -α*delta\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
