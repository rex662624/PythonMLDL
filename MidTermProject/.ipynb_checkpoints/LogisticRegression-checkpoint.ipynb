{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(Y): \n",
    "    if Y<=5:\n",
    "        return 1\n",
    "    elif Y <=10:\n",
    "        return 2\n",
    "    elif Y <=15:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4 \n",
    "def Now_classify(Y,who):\n",
    "    if Y == who:\n",
    "        #print('p')\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "def PredictProbability(data):\n",
    "    if data>=0.5:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    g = np.zeros((z.shape[0],1))\n",
    "    #print(z.shape)\n",
    "    g = 1 / (1+np.exp(-z))\n",
    "    assert(g.shape==z.shape)\n",
    "    return g\n",
    "\n",
    "def CostFunction_Logi(θ,x,y): #Cost function\n",
    "    if(θ.ndim==1):\n",
    "        θ=θ.reshape(θ.shape[0],1)\n",
    "    #print(θ.shape)\n",
    "    #print(θ)\n",
    "    #print(x.shape)\n",
    "    #print(y.shape)\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    assert(θ.shape[0]==x.shape[1])#先確定他們的dim一樣\n",
    "    h = Sigmoid(x @ θ) #這是我們的hypothesis 也就是 θ0X0+θ1X1......θnXn = y 用 matmul，矩陣乘法一次算出全部\n",
    "    \n",
    "    return(float(sum(-y * np.log(h) - (1-y) * np.log(1-h))/(m))) #multiply是elementwise 的乘法 (或用float((((h-y).T @ (h-y))/(2*m))))\n",
    "\n",
    "def GradientDescent_Logi(x,y,θ,α,num_iters):\n",
    "    m = x.shape[0] # training set 共有幾組\n",
    "    J = np.zeros((num_iters,1))#我們把每一次的J都存起來，因為要是gradient decent是成功的，每輪的J應該會逐漸下降，不會上升\n",
    "    for iter in range(1,num_iters):#做num_iters次更新\n",
    "        delta = x.T @ ( Sigmoid(x @ θ) - y )   /m \n",
    "        θ = θ - α * delta\n",
    "        #print(CostFunction(θ,x,y))\n",
    "        J[iter][0]=CostFunction_Logi(θ,x,y)\n",
    "    return (θ , J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticR_CallFunc(X_logi_train,X_logi_test,Y_logi_train,Y_logi_test): \n",
    "\n",
    "    logistic_regr = linear_model.LogisticRegression()\n",
    "    logistic_regr.fit(X_logi_train,Y_logi_train.values.ravel())\n",
    "\n",
    "\n",
    "    Y_logi_Predict = logistic_regr.predict(X_logi_test)\n",
    "    count = 0\n",
    "    right = 0\n",
    "    \n",
    "    for i in Y_logi_test.index :\n",
    "        #print(Y_logi_test.loc[i,'Rings'])\n",
    "        #print(Y_logi_Predict[count])\n",
    "        if((Y_logi_Predict[count])==(Y_logi_test.loc[i,'Rings'])):\n",
    "            right=right+1 \n",
    "        count=count+1\n",
    "\n",
    "    print(f\"{Y_logi_test.size} 個 testing data 中有 {Y_logi_test.size-right} 個 data 預測失誤 正確率 {100 *right/Y_logi_test.size} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
